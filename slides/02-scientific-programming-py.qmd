---
title: "Introduction to HPC"
subtitle: "Scientific Programming with Python"

author:
  - name: Manuel Holtgrewe
    orcid: 0000-0002-3051-1763
    affiliations:
      - ref: bihealth
affiliations:
  - id: bihealth
    name: Berlin Institute of Health at Charité
    address: Charitéplatz 1,
    postal-code: 10117
    city: Berlin
    country: Germany
title-slide-attributes:
  data-background-size: contain
  data-background-image: themes/bih/bih_bg_logo.png
format:
  revealjs:
    theme:
      - default
      - themes/bih/theme.scss
    slide-number: true
    navigation-mode: linear
    controls-layout: bottom-right
    controls-tutorial: false
    smaller: true
    self-contained: true
    # Override default "normal" size to get 16:9 aspect ratio.
    width: 1200
    height: 675
    margin: 0.05
    # Enable local plugins.
    revealjs-plugins:
      - plugins/fullscreen
---

## Session Overview

__Aims__

- Learn about important applications for Python in scientific programming ...
- ... in the biomedical/life sciences.

__Considered Applications__

- "Data ~~science~~ wrangling" with [polars](https://www.pola.rs/)
- Fast numerical computations with SciPy/Numpy
- Interfacing with R for Statistics
- Machine learning with [scikit-learn](https://scikit-learn.org/)
- Deep Learning with Tensorflow

## Jupyter Notebooks {.inverse background-color="#70ADC1"}

- Introduction
- Installation and Usage

## Introduction

:::: {.columns}

::: {.column width="50%"}

- Jupyter Notebooks are a web-based interactive computing environment
    - Jupyter Lab is the next generation
- You edit **cells** with python code which is executed in a **kernel**
- It's best explained by example ;-)

:::

::: {.column width="50%"}

![](img/02-scientific-programming-py/wikipedia-jupyter-notebook.png)

:::

::::

## Installation

- Jupyter is installed as a Python module.
- To install it locally with conda

  ```bash
  mamba create -y -n jupyter-example jupyterlab
  conda activate jupyter-example
  jupyter lab # OR: jupyter notebook
  # OUTPUT:
  ...
  http://127.0.0.1:8888/?token=a5eea49bec7cd70538c21e947e2de596a3ab66b39506ee72
  ...
  ```

- [More Kernels](https://github.com/jupyter/jupyter/wiki/Jupyter-kernels) available
    - Bash
    - R
    - Julia
    - ...

## Using Jupyter (1)

:::: {.columns}

::: {.column width="50%"}

- There is no magic!
- Packages can be installed in the conda environment (or via pip)
    - `mamba install -y plotly_express`
- The kernel keeps the current state
    - To ensure that you have a clean state
    - restart kernel and rerun all cells
- Bonus: `ipython` is a turbo-charged interpreter on the command line
- Useful: Markdown cells for documentation

:::

::: {.column width="50%"}

![](img/02-scientific-programming-py/jupyter-lab-plotly.png)

:::

::::

## Using Jupyter (2)

:::: {.columns}

::: {.column width="50%"}

**Jupyter Lab vs. Notebook**

- Originally: `ipython` notebook
- Jupyter Notebook: single file
- Jupyter Lab: IDE-like, terminal, ...

:::

::: {.column width="50%"}

**Remarks**

- `.ipynb` files are JSON files
- They contain all code but also all output and log output (stderr)
- To clear all output: `jupyter nbconvert --clear-output --to notebook --output=OUTPUT INPUT.ipynb`

:::

::::

## Using Jupyter (3)

**Advantages**

- Really nice to try out things interactively
- Excellent for interactive data analysis / visualization

**Drawbacks**

- Code cannot be reused (use Python modules for this)
- Version control with Git feasible but not ideal

## Using Jupyter on the HPC

- Super easy to get started locally!
- Also easy to launch a job on the HPC, but ...
    - how to connect to its web interface?
- Options:
    - SSH tunneling
    - Open OnDemand Portal ([hpc-portal.cubi.bihealth.org](https://hpc-portal.cubi.bihealth.org))

## Data Wrangling with Polars {.inverse background-color="#70ADC1"}

- Introduction
- Loading and Writing Data
- "Tidy Data" with Polars
- Data Visualization

## Introduction

:::: {.columns}

::: {.column width="50%"}

**What is Polars?**

- Polars is a DataFrame library for Python (written in Rust)
    - `mamba install -y polars`
- It is (almost a) drop-in replacement for Pandas
- Think "super-fast R-like data frames in Python"
    - also available for Rust, R, NodeJS
- [Python Documentation](https://pola-rs.github.io/polars/py-polars/html/reference/index.html)

:::

::: {.column width="50%"}

**Polars vs. Pandas**

- Pandas is the original library and _de-facto_ standard
- Nobody is particularly happy with Pandas (including its author)
    - In particular with **Pandas indexing**
- Polars is a new library that tries to fix several issues
    - Polars has no indexing ;-)

:::

::::

## Loading and Writing Data (1)

```
In [1]: import polars as pl
   ...: iris_data = pl.read_csv("https://gist.githubusercontent.com/netj/8836201/raw/6f9306ad21398ea43cba4f7d537619d0e07d5ae3/iris.csv")
   ...: iris_data.head()
Out[1]:
shape: (5, 5)
┌──────────────┬─────────────┬──────────────┬─────────────┬─────────┐
│ sepal.length ┆ sepal.width ┆ petal.length ┆ petal.width ┆ variety │
│ ---          ┆ ---         ┆ ---          ┆ ---         ┆ ---     │
│ f64          ┆ f64         ┆ f64          ┆ f64         ┆ str     │
╞══════════════╪═════════════╪══════════════╪═════════════╪═════════╡
│ 5.1          ┆ 3.5         ┆ 1.4          ┆ 0.2         ┆ Setosa  │
│ 4.9          ┆ 3.0         ┆ 1.4          ┆ 0.2         ┆ Setosa  │
│ 4.7          ┆ 3.2         ┆ 1.3          ┆ 0.2         ┆ Setosa  │
│ 4.6          ┆ 3.1         ┆ 1.5          ┆ 0.2         ┆ Setosa  │
│ 5.0          ┆ 3.6         ┆ 1.4          ┆ 0.2         ┆ Setosa  │
└──────────────┴─────────────┴──────────────┴─────────────┴─────────┘
```

## Loading and Writing Data (2)

```
In [2]: iris_data.write_csv("/tmp/out.tsv", separator="\t")

In [3]: with open("/tmp/out.tsv") as inputf:
   ...:     print(inputf.readline())
   ...:     print(inputf.readline())
   ...:
sepal.length    sepal.width     petal.length    petal.width     variety

5.1     3.5     1.4     0.2     Setosa


```

## Tidy Data with Polars (1)

- Hadley Wickham wrote a whole [paper about tidy data](https://vita.had.co.nz/papers/tidy-data.pdf) - he is probably the "most influential R guy"
- Tidy data is a way to organize data in a tabular format
    - each variable is a column
    - each observation is a row
    - each type of observational unit is a table
- This section is based on [Kevin Heavey's M"Modern Polars"](https://kevinheavey.github.io/modern-polars/tidy.html)

## Tidy Data with Polars (2)

Original data:

```
date,away_team,away_points,home_team,home_points
"Fri, Apr 1, 2016",Philadelphia 76ers,91,Charlotte Hornets,100
"Fri, Apr 1, 2016",Dallas Mavericks,98,Detroit Pistons,89
"Fri, Apr 1, 2016",Brooklyn Nets,91,New York Knicks,105
"Fri, Apr 1, 2016",Cleveland Cavaliers,110,Atlanta Hawks,108
"Fri, Apr 1, 2016",Toronto Raptors,99,Memphis Grizzlies,95
```

## Tidy Data with Polars (3)

Load and clean the data.

```python
import polars as pl
games_pl = (
    pl.read_csv("nba.csv")
    .filter(~pl.all(pl.all().is_null()))
    .with_columns(
        pl.col("date").str.strptime(pl.Date, "%a, %b %d, %Y"),
    )
    .sort("date")
    .with_row_count("game_id")
)
games_pl.head()
```

Output

```
┌─────────┬────────────┬─────────────────────┬─────────────┬───────────────────┬─────────────┐
│ game_id ┆ date       ┆ away_team           ┆ away_points ┆ home_team         ┆ home_points │
│ ---     ┆ ---        ┆ ---                 ┆ ---         ┆ ---               ┆ ---         │
│ u32     ┆ date       ┆ str                 ┆ i64         ┆ str               ┆ i64         │
╞═════════╪════════════╪═════════════════════╪═════════════╪═══════════════════╪═════════════╡
│ 0       ┆ 2016-04-01 ┆ Philadelphia 76ers  ┆ 91          ┆ Charlotte Hornets ┆ 100         │
│ 1       ┆ 2016-04-01 ┆ Dallas Mavericks    ┆ 98          ┆ Detroit Pistons   ┆ 89          │
│ 2       ┆ 2016-04-01 ┆ Brooklyn Nets       ┆ 91          ┆ New York Knicks   ┆ 105         │
│ 3       ┆ 2016-04-01 ┆ Cleveland Cavaliers ┆ 110         ┆ Atlanta Hawks     ┆ 108         │
│ 4       ┆ 2016-04-01 ┆ Toronto Raptors     ┆ 99          ┆ Memphis Grizzlies ┆ 95          │
└─────────┴────────────┴─────────────────────┴─────────────┴───────────────────┴─────────────┘
```

## Data Operations

- Polars (just as Pandas and Hadley's tidyverse for R) has a rich set of operations
    - [Cheat Sheet](https://franzdiebold.github.io/polars-cheat-sheet/Polars_cheat_sheet.pdf)
    - `pivot` - "long" to "wide" format
    - `melt` - "wide" to "long" format
    - `join` - join two tables
    - ... much more!
- Suggested exercise for later:
    - Go through [Tidy data section of Modern Polars](https://kevinheavey.github.io/modern-polars/tidy.html)
    - Try to setup a new conda environment, launch Jupyter through OnDemand Portal, go through the steps

## Data Visualization (1)

:::: {.columns}

::: {.column width="50%"}

- **Plotly Express** is a high-level plotting library for Python
    - It is based on [Plotly](https://plotly.com/python/) which is more low-level
    - This is really nice to create interactive plots!
- Suggested exercise for later:
    - Go through [Plotly Express Tutorial](https://plotly.com/python/plotly-express/)
    - Again, try it in Jupyter via OnDemand Portal

:::

::: {.column width="50%"}

![](img/02-scientific-programming-py/plotly-express-example.png)

:::

::::

## Data Visualization (2)

:::: {.columns}

::: {.column width="50%"}

- **Vega** is a *declarative language* for data visualization
    - It is used by [Altair](https://altair-viz.github.io/) which is a Python library
    - It is also used by [Vega Lite](https://vega.github.io/vega-lite/) which is a JavaScript library
    - Vega Lite is used by [Voyager](https://vega.github.io/voyager/) which is a web-based data exploration tool
- This is more advanced but also very powerful
    - [Here are the official tutorials](https://vega.github.io/vega/tutorials/)

:::

::: {.column width="50%"}

![](img/02-scientific-programming-py/vega-altair-example.png)

:::

::::

## Data Visualization (3)

:::: {.columns}

::: {.column width="50%"}

- **Bokeh** is a Python library for interactive data visualization
    - An alternative to Plotly Express
- [Here are the official tutorials](https://mybinder.org/v2/gh/bokeh/bokeh-notebooks/master?filepath=tutorial%2F00%20-%20Introduction%20and%20Setup.ipynb)

:::

::: {.column width="50%"}

![](img/02-scientific-programming-py/bokeh-example.png)

:::

::::

## Data Visualization (4)

- **Matplotlib** is the "original" plotting library for Python
    - it offers pretty low-level plotting functions only
    - [Seaborn](https://seaborn.pydata.org/) adds higher level plotting functions
- [Here is the matplotlib tutorial](https://matplotlib.org/stable/tutorials/index.html)
- Historically, it can be seen as part of the "MATLAB functionality for Python" effort
    - 1995 NumPy: fast numerics for Python
    - 2001 SciPy: scientific functions for Python, builds on NumPy
    - 2003 Matplotlib: plotting for Python, builds on NumPy

## Conversion Between Polars and Pandas (1)

:::: {.columns}

::: {.column width="50%"}

**But - why?!**

- Polars is a new library
- Pandas is the de-facto standard
- Certain third-party libraries may not work with Pandas yet
    - e.g., `scikit-learn`
    - e.g., `rpy2`

:::

::: {.column width="50%"}

**Installing dependencies**

```
mamba create -y -n polars-example \
    polars pandas pyarrow
```

:::

::::

## Conversion Between Polars and Pandas (2)

**Polars to Pandas**

```python
In [1]: import polars as pl
   ...: import pandas as pd
   ...:
   ...: pl_data = pl.DataFrame({"name": ["alice", "bob"], "age": [23, 52]})
   ...: pd_data = pl_data.to_pandas()
   ...:

In [2]: type(pl_data)
Out[2]: polars.dataframe.frame.DataFrame

In [3]: type(pd_data)
Out[3]: pandas.core.frame.DataFrame
```

## Conversion Between Polars and Pandas (3)

**Pandas to Polars**

```python
In [1]: import polars as pl
   ...: import pandas as pd
   ...:
   ...: pd_data = pd.DataFrame({"name": ["alice", "bob"], "age": [23, 52]})
   ...: pl_data = pl.from_pandas(pd_data)

In [2]: type(pd_data)
Out[2]: pandas.core.frame.DataFrame

In [3]: type(pl_data)
Out[3]: polars.dataframe.frame.DataFrame
```

## Numerical Computation with SciPy/NumPy {.inverse background-color="#70ADC1"}

- Introduction
- Input and Output
- Arrays and Shapes
- Vectorized Operations

## Introduction to SciPy/NumPy

:::: {.columns}

::: {.column width="50%"}

**NumPy**

- Numerical computing data structures
    - high-performance vectors/matrices
- Numerical computing algorithms
    - vectorized operations, random numbers ...
    - linear algebra

:::

::: {.column width="50%"}

**SciPy**

- Fundamental scientific algorithms
    - clustering
    - interpolation / smoothing
    - statistics
    - sparse matrices
    - image and signal processing
    - FFT, integration

:::

::::

👉 Mostly low level characteristics, many other libraries build on top.

## Input and Output (1)

Saving single array to `.npy` format

```python
import numpy as np

mat = np.mat("1 2; 3 4")
np.save("mat.npy", mat)
```

Saving multiple arrays to `.npz` format (ZIP of `.npy`)

```python
import numpy as np

arr = np.array([1, 2, 3, 4])
mat = np.mat("1 2; 3 4")
np.savez("data.npz", arr=arr, mat=mat)
```

## Input and Output (2)

Loading single array to `.npy` format

```python
In [1]: import numpy as np
   ...:
   ...: mat = np.load("mat.npy")
   ...: mat
Out[1]:
array([[1, 2],
       [3, 4]])
```

Loading multiple arrays from `.npz`:

```python
In [1]: import numpy as np
   ...:
   ...: data = np.load("data.npz")
   ...:

In [2]: data.keys()
Out[2]: KeysView(NpzFile 'data.npz' with keys: arr, mat)

In [3]: data["arr"]
Out[3]: array([1, 2, 3, 4])

In [4]: data["mat"]
Out[4]:
array([[1, 2],
       [3, 4]])
```

## Arrays and Shapes

- arrays are the fundamental data structure
- the `shape` attribute determines the dimension

```python
In [1]: import numpy as np

In [2]: arr = np.array([1, 2, 3, 4])

In [3]: arr
Out[3]: array([1, 2, 3, 4])

In [4]: arr.shape
Out[4]: (4,)

In [5]: arr.shape = (2, 2)

In [6]: arr
Out[6]:
array([[1, 2],
       [3, 4]])
```

## Vectorized Operations (1)

- vectorized operations are the key to fast numerical computations
- they are implemented in C and Fortran

```python
In [1]: import numpy as np
   ...:
   ...: a = np.array([1,2,3,4])
   ...: b = np.array([5,6,7,8])
   ...:

In [2]: np.add(a, b)
Out[2]: array([ 6,  8, 10, 12])

In [3]: np.multiply(a, b)
Out[3]: array([ 5, 12, 21, 32])
```

## Vectorized Operations (2)

**Plain Python**

```python
In [1]: import timeit
   ...:
   ...: a = range(100_000_000)
   ...: b = range(100_000_000)
   ...:
   ...: timeit.timeit("[x + y for x, y in zip(a, b)]", globals=globals(), number=1)
   ...:
Out[1]: 8.240364263998345
```

**With numpy**

```python
In [1]: import numpy as np
   ...: import timeit
   ...:
   ...: npa = np.arange(100_000_000)
   ...: npb = np.arange(100_000_000)
   ...:
   ...: timeit.timeit("np.add(npa, npb)", globals=globals(), number=1)
   ...:
Out[1]: 0.16427204399951734
```

## SciPy Example

```python
In [1]: import numpy as np
   ...: from scipy import linalg
   ...:
   ...: A = np.array([[1,3,5],[2,5,1],[2,3,8]])
   ...: A
Out[1]:
array([[1, 3, 5],
       [2, 5, 1],
       [2, 3, 8]])

In [2]: linalg.inv(A)
Out[2]:
array([[-1.48,  0.36,  0.88],
       [ 0.56,  0.08, -0.36],
       [ 0.16, -0.12,  0.04]])

In [3]: A.dot(linalg.inv(A)) #double check
Out[3]:
array([[ 1.00000000e+00, -1.11022302e-16,  4.85722573e-17],
       [ 3.05311332e-16,  1.00000000e+00,  7.63278329e-17],
       [ 2.22044605e-16, -1.11022302e-16,  1.00000000e+00]])
```

- For more information [see documentation](https://docs.scipy.org/doc/scipy/tutorial/index.html#user-guide)

## Summary / Exercise

**Summary**

- NumPy provides fast numerical data structures and operations
- SciPy provides scientifc functions / algorithms
- By handing the operations to this C/Fortran code, the code will run faster than native Python code

**Potential Exercises**

- Implement your own vectorized function ([How-To](https://numpy.org/doc/stable/reference/generated/numpy.vectorize.html))
- Try out the random number generator
- Use the statistics functions such as `average`, `quantile`, etc.

## Interfacing with R for Statistics {.inverse background-color="#70ADC1"}

- Introduction
- Low-Level Approaches
- Using `rpy2`

## Introduction

:::: {.columns}

::: {.column width="65%"}
**Why interface to R with Python?**

- Python is an excellent general purpose language
    - great for scientific programming
    - increasing support for statistics (`statsmodels`, `linearmodels`, ...)
- R is the *de facto* standard for statistics
    - all the methods, all the features
    - excellent machine learning, biostatistics, ...
:::

::: {.column width="35%"}
**`rpy2` Installation**

```
mamba create -y -n rpy2-example \
    python=3.10 rpy2 jupyterlab pandas \
    pyarrow polars
conda activate rpy2-example
```

:::

::::

## Low-Level Approaches

Maybe the following is enough:

1. write out data to disk, .e.g, as CSV/TSV files
2. call an R script from Python with `subprocess.call`
    - R script writes results to disk
3. read in data from disk again

If so - do it!

If you need more advanced features, stay tuned!

## Using `rpy2` (1)

Hello ~~World~~ Pi!

```
$ R --vanilla

> pi
[1] 3.141593
```

In Python:

```python
In [1]: from rpy2 import robjects
   ...: robjects.r["pi"]
Out[1]:
<rpy2.robjects.vectors.FloatVector object at 0x7f66d67a5800> [RTYPES.REALSXP]
R classes: ('numeric',)
[3.141593]
```

## Using `rpy2` (2)

Student's t-test

```python
In [1]: import rpy2.robjects as robjects
   ...: from rpy2.robjects.packages import importr
   ...:
   ...: stats = importr('stats')
   ...:
   ...: group1 = robjects.FloatVector([23.5, 25.1, 28.3, 29.8, 30.5])
   ...: group2 = robjects.FloatVector([20.1, 22.5, 25.3, 27.9, 29.2])
   ...:
   ...: result = stats.t_test(group1, group2)
   ...:
   ...: print(result)

        Welch Two Sample t-test

data:  c(23.5, 25.1, 28.3, 29.8, 30.5) and c(20.1, 22.5, 25.3, 27.9, 29.2)
t = 1.1311, df = 7.656, p-value = 0.2922
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -2.573733  7.453733
sample estimates:
mean of x mean of y
    27.44     25.00

In [2]: result
Out[2]:
<rpy2.robjects.vectors.ListVector object at 0x7f668c5796c0> [RTYPES.VECSXP]
R classes: ('htest',)
[FloatSexp..., FloatSexp..., FloatSexp..., FloatSexp..., ..., FloatSexp..., StrSexpVe..., StrSexpVe..., StrSexpVe...]
  statistic: <class 'rpy2.robjects.vectors.FloatVector'>
  <rpy2.robjects.vectors.FloatVector object at 0x7f668c6a3700> [RTYPES.REALSXP]
R classes: ('numeric',)
[1.131085]
  parameter: <class 'rpy2.robjects.vectors.FloatVector'>
  <rpy2.robjects.vectors.FloatVector object at 0x7f668c700b80> [RTYPES.REALSXP]
R classes: ('numeric',)
[7.656022]
  p.value: <class 'rpy2.robjects.vectors.FloatVector'>
  <rpy2.robjects.vectors.FloatVector object at 0x7f668c700f80> [RTYPES.REALSXP]
R classes: ('numeric',)
[0.292203]
  conf.int: <class 'rpy2.robjects.vectors.FloatVector'>
  <rpy2.robjects.vectors.FloatVector object at 0x7f668c57ad00> [RTYPES.REALSXP]
R classes: ('numeric',)
[-2.573733, 7.453733]
...
  null.value: <class 'rpy2.robjects.vectors.FloatVector'>
  <rpy2.robjects.vectors.FloatVector object at 0x7f668c579780> [RTYPES.REALSXP]
R classes: ('numeric',)
[2.157220]
  stderr: <class 'rpy2.robjects.vectors.StrVector'>
  <rpy2.robjects.vectors.StrVector object at 0x7f668c6c1100> [RTYPES.STRSXP]
R classes: ('character',)
['two.sided']
  alternative: <class 'rpy2.robjects.vectors.StrVector'>
  <rpy2.robjects.vectors.StrVector object at 0x7f668c549ac0> [RTYPES.STRSXP]
R classes: ('character',)
['Welch Two Sample t-test']
  method: <class 'rpy2.robjects.vectors.StrVector'>
  <rpy2.robjects.vectors.StrVector object at 0x7f668c5499c0> [RTYPES.STRSXP]
R classes: ('character',)
['c(23.5, 25.1, 28.3, 29.8, 30.5) and c(20.1, 22.5...]
```

## Using `rpy2` (3)

**From Pandas to R**

```python
In [1]: import pandas as pd
   ...: import rpy2.robjects as ro
   ...: from rpy2.robjects.packages import importr
   ...: from rpy2.robjects import pandas2ri
   ...:
   ...: pd_df = pd.DataFrame({'int_values': [1,2,3],
   ...:                       'str_values': ['abc', 'def', 'ghi']})
   ...:
   ...: with (ro.default_converter + pandas2ri.converter).context():
   ...:   r_from_pd_df = ro.conversion.get_conversion().py2rpy(pd_df)
In [2]: base = importr('base')
   ...:
   ...: with (ro.default_converter + pandas2ri.converter).context():
   ...:   df_summary = base.summary(pd_df)
   ...: print(df_summary)
   int_values   str_values
 Min.   :1.0   Length:3
 1st Qu.:1.5   Class :character
 Median :2.0   Mode  :character
 Mean   :2.0
 3rd Qu.:2.5
 Max.   :3.0
```

## Using `rpy2` (4)

**From R to Pandas**

```python
In [1]: import pandas as pd
   ...: import rpy2.robjects as ro
   ...: from rpy2.robjects.packages import importr
   ...: from rpy2.robjects import pandas2ri
   ...:
   ...: r_df = ro.DataFrame({'int_values': ro.IntVector([1,2,3]),
   ...:                      'str_values': ro.StrVector(['abc', 'def', 'ghi'])})

In [2]: base = importr('base')
   ...:
   ...: with (ro.default_converter + pandas2ri.converter).context():
   ...:   df_summary = base.summary(r_df)
   ...: print(df_summary)
   int_values   str_values
 Min.   :1.0   Length:3
 1st Qu.:1.5   Class :character
 Median :2.0   Mode  :character
 Mean   :2.0
 3rd Qu.:2.5
 Max.   :3.0
```

## Using `rpy2` (5)

**Installing Packages**

```python
import rpy2.robjects.packages as rpackages
utils = rpackages.importr("utils")
utils.chooseCRANmirror(ind=1) # select the first mirror in the list

from rpy2.robjects.vectors import StrVector
packnames = ("ggplot2", "lattice", "lazyeval")
names_to_install = [x for x in packnames if not rpackages.isinstalled(x)]
if len(names_to_install) > 0:
    utils.install_packages(StrVector(names_to_install))
```

## Using `rpy2` (6)

**Plotting with ggplot2**

```python
import numpy as np
import pandas as pd
import rpy2.robjects.packages as packages
import rpy2.robjects.lib.ggplot2 as ggplot2
import rpy2.robjects as ro

R = ro.r
datasets = packages.importr("datasets")
mtcars = packages.data(datasets).fetch("mtcars")["mtcars"]

gp = ggplot2.ggplot(mtcars)
pp = (gp 
      + ggplot2.aes_string(x="wt", y="mpg")
      + ggplot2.geom_point(ggplot2.aes_string(colour="qsec"))
      + ggplot2.scale_colour_gradient(low="yellow", high="red") 
      + ggplot2.geom_smooth(method="auto") 
      + ggplot2.labs(title="mtcars", x="wt", y="mpg"))

pp.plot()
R("dev.copy(png,'/tmp/out.png')")
```

## Machine Learning `scikit-learn` {.inverse background-color="#70ADC1"}

- Introduction
- Estimator Cheat Sheet
- Example: Clustering
- Example: Regression
- Example: Classification

## Introduction

## Estimator Cheat Sheet

## Example: Clustering

## Example: Regression

## Example: Classification

- Introduction
- Installation
- "TensorFlow 2 Quickstart for Beginners"
- Running on the HPC

## Introduction

## Installation

## "TensorFlow 2 Quickstart for Beginners"

https://www.tensorflow.org/tutorials/quickstart/beginner

## Running on the HPC

## Deep Learning with Tensorflow {.inverse background-color="#70ADC1"}

## Bring Your Own Project {.inverse background-color="#70ADC1"}

🫵 Where can you apply what you have learned in your PhD project?

## This is not the end...

... but all for this session

__Recap__

- Overview of Python in scientific programming
- Data wrangling with Polars
- Numerical computations with SciPy/NumPy
- Interfacing with R for Statistics
- Machine Learning with `scikit-learn`
- Deep Learning with Tensorflow
