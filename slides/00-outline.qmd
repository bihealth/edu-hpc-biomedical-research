---
title: "Introduction to High-Performance Compute"
subtitle: "Overview"
author: "Manuel Holtgrewe"
institute: "Core Unit Bioinformatics, Berlin Institute of Health"
format:
  revealjs:
    slide-number: true
    navigation-mode: vertical
    controls-layout: bottom-right
    controls-tutorial: false
    include-in-header:
      text: |
        <style>
        .center-h {
          text-align: center;
        }
        </style>
---

# Course Overview

- Welcome to the course! üëã
- Introduction to High-Performance Computing (HPC)
- Focus on biomedical and medical research applications

::: {.center-h}
üî¨ ‚å®Ô∏è üß¨
:::

. . .

- Duration: [duration of the course]
- Instructor: Manuel Holtgrewe
- Contact Information: manuel.holtgrewe@bih-charite.de

# Course Objectives

:::: {.columns}

::: {.column width="60%"}
- Fundamentals of HPC ...
- ... for biomedical research
- Practical skills in
    - Linux command line
    - HPC job submission
    - Scientific programming
- Parallel computing techniques and their applications
:::

::: {.column width="40%"}
![](img/00-outline/hpc-unsplash-jkuR9QteDGY.jpg)
:::

::::

# Participant Background ü§∏

- Briefly introduce yourself:
  - Name
  - Background (biomedical/computational, programming experience, etc.)
  - Expectations from the course

# What is High-Performance Computing?

- Attempt at a definition
- Role of HPC in biomedical and medical research
- Trade-Offs

## Attempt at a Definition

HPC ...

::: {.incremental}

- refers to [advanced computing techniques]{.fragment .highlight-red} & technologies to solve [complex computational problems]{.fragment .highlight-red} efficiently
- involves leveraging [parallel processing]{.fragment .highlight-red}, [large-scale data analysis]{.fragment .highlight-red}, and [specialized hardware]{.fragment .highlight-red}
    - to achieve [high computational performance]{.fragment .highlight-red}
- systems consist of [multiple computing nodes]{.fragment .highlight-red} connected through a [high-speed network]{.fragment .highlight-red}, [working together]{.fragment .highlight-red}
- enables [researcher]{.fragment .highlight-red} to tackle [computationally intensive tasks]{.fragment .highlight-red} that would be infeasible or too time-consuming otherwise
- finds applications in a [wide range of fields]{.fragment .highlight-red}, including scientific research, engineering, data analytics, and machine learning

:::

## Role of HPC in Biomedical Research

HPC ...

::: {.incremental}

- ... plays a crucial role by enabling tacking computational challenges
- ... allows for analyzing large-scale genomics, proteomics, ..., datasets
    - leading to insights into diseases and potential treatments
- ... facilitates simulations such as protein folding, molecule interactions, etc.
- ... enables the efficient training of large-scale statistical and machine learning models

:::

## Trade-Offs of HPC

::::: {.incremental}

:::: {.columns}

::: {.column width="50%"}
### Advantages

- fast execution of complex computational tasks
- process and analyze large data sets
- fast and large storage systems
- [MORE POWER]{.fragment .highlight-red} ü¶æ
:::

::: {.column width="50%"}
### Drawbacks

- learning curve / entry barrier
- usually shared with other users
- expensive to buy/operate
- high power usage/CO<sub>2</sub> footprint ([reference](https://www.bcs.org/articles-opinion-and-research/carbon-footprint-the-not-so-hidden-cost-of-high-performance-computing/))
- ["why is my job killed/crashing/not running?"]{.fragment .highlight-red} üò∂‚Äçüå´Ô∏è
:::

::::

:::::

<hr>

There is no free lunch!

## What is Your Take? ü§∏

__"Blitzlicht" Instructions__

- answer one of the questions
- do not repeat a previous answer

. . .

__Questions__

- which plus/minus points have you hit so far?
- what do you hope / fear?

# HPC Systems and Architecture

- Overview of HPC systems and architecture:
  - Compute nodes
  - Shared memory vs. distributed memory systems
  - Interconnects and network topology
  - Job schedulers and resource management

## Compute Nodes (1)

"Same-same (as your laptop), but different."

::::: {.columns}

:::: {.column width="50%"}

::: {.incremental}

- 2+ sockets with
    - many-cores CPUs
    - e.g., 2 x 24 x 2 = 96 threads
- high memory (e.g., 200+ GB)
- fast network interface card
    - "legacy": 10GbE (x2)
    - modern: 25GbE (x2)

- local disks
    - HDD or solid state SSD/NVME
:::

::::

:::: {.column width="50%"}

::: {.r-stack}
![](img/00-outline/hpc-node-photo.jpg)

![](img/00-outline/hpc-node-schematics.png){.fragment}
<!-- Figure source: Haarhoff, Daniel, and Lukas Arnold. "Performance analysis and shared memory parallelisation of FDS." Proceedings of Fire and Evacuation Modeling Technical Conference. 2014. -->
:::

::::

:::::

## Compute Nodes (2)

More differences from "consumer-grade" hardware:

::: {.incremental}

- error correcting memory (bit flips are real)
- stronger fans
- redundant power control
- redundant disks
:::

::: {.callout-tip title="You are not the admin"}
no root/admin access, no `sudo`
:::

## Shared vs. Distributed Memory

::::: {.incremental}

:::: {.columns}

::: {.column width="50%"}
### Shared Memory

- aka "in-core" parallel
- aka "multi-threading"

```{mermaid}
graph BT
    sq1[thread 1] --> ci(memory address)
    sq2[thread 2] --> ci(memory address)
```

- ‚ûï low overhead
- ‚ûï easy to get started
- ‚ûñ implicit communication, easy to make errors
- ‚ûñ do you __really__ understand your memory model?

:::

::: {.column width="50%"}
### Distributed Memory

- aka "out-of-core" parallelism
- aka "message-passing"

```{mermaid}
graph LR
    sq1[thread 1] -->|How are you?| sq2[thread 2]
    sq2[thread 2] -->|- fine, thank!| sq1[thread 1]
```

- ‚ûï explicit communication, fewer wrong assumptions(?)
- ‚ûï model scales up better for larger systems
- ‚ûñ harder to get started
- ‚ûñ more complex primitives

:::

::::

:::::

## What is Your Experience? ü§∏

__"Blitzlicht" Instructions__

- answer both of the questions
- do not repeat a previous answer exactly

. . .

__Questions__

- have you used shared/distributed memory parallelism before?
- what is your experience/hope?

## Node Interconnect

<!-- continue here-->

- single nodes useless
- interconnect important
- storage nodes "behind the scenes"
- scheduler etc. "behind the scenes"

## Cluster Architecture Overview

- head nodes
    - login node
    - transfer nodes
- compute nodes
    - generic: cpu
    - specialized: high-mem/gpus
- different features (cpu instruction sets, ...)

## Job Scheduler and Resource Management

- resources: cpu/memory/gpu
- distributed in time slicing manner
- knows all resources
- assigns them to users

# Linux Command Line and SSH

- Importance of Linux command line for HPC
- Basics of using the command line
- Introduction to SSH for remote access to HPC systems

# Scientific Programming Tools

- Introduction to programming tools for scientific computing:
  - Python: NumPy, ~~Pandas~~ Polars, scikit-learn
  - R: data.table, dplyr, caret
- Overview of their applications in biomedical and medical research

# Course Structure and Schedule

- Overview of the course structure:
  - Section 1: Introduction to HPC
  - Section 2: Linux Command Line and SSH
  - Section 3: Bash Shell and Scripting
  - Section 4: SLURM Job Scheduler
  - Section 5: Scientific Programming with Python/R
  - Optional: Section 6: Advanced Topics
- Schedule and duration for each section

# Icebreaker Activity

- Let's get to know each other!
- Icebreaker activity instructions

# Questions and Discussion

- Open the floor for any questions or discussion
- Encourage participants to ask questions or share their thoughts

# Conclusion and Next Steps

- Recap of the topics covered in this session
- Next steps: Upcoming sessions and what to expect
- Thank participants for their participation

# Thank You and Goodbye

- Thank participants for attending the introductory session
- Provide contact information for further inquiries or support
- Express excitement for the upcoming sessions

